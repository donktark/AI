# LSTM Text Generation
> [ğŸ”—LSTM í…ìŠ¤íŠ¸ ìƒì„±](https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/)   
[ğŸ”—Winnie-the-Pooh í…ìŠ¤íŠ¸ íŒŒì¼](https://www.gutenberg.org/ebooks/67098)
<pre>
<b>  ì‘ì—… í™˜ê²½ </b> 
OS      : Ubuntu Linux 20.04
CPU     : 8
Memory  : 64GB
GPU     : ë¹„í™œì„±í™”
</pre>
## ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
import numpy as np
import os 
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
```
## ì†Œë¬¸ì ë³€í™˜ ë° ë‹¨ì–´ì‚¬ì „ ì¶”ì¶œ
```python
# íŒŒì¼ ì½ê¸°, ì†Œë¬¸ìë¡œ ë³€í™˜
file_name ='./Winnie-the-Pooh.txt'
raw_text = open(file_name, 'r', encoding='utf-8').read()
lower_text = raw_text.lower()

# chars -> integer mapping
chars = sorted(list(set(lower_text))) 
chars_to_int = dict((c, i) for i, c in enumerate(chars))
print(chars_to_int) #íŒŒì¼ ë‚´ì— ìˆëŠ” ê³ ìœ  ë¬¸ìë¥¼ ë‹¨ì–´ì‚¬ì „ìœ¼ë¡œ ì €ì¥í•¨

n_chars = len(lower_text)
n_vocab = len(chars)
print("Total Characters: ", n_chars)
print("Total Vocab: ", n_vocab)
```
íŒŒì¼ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•œ ë’¤ì— ê³ ìœ í•œ ë¬¸ìì—´ í•˜ë‚˜ì”© ê°ê°ì˜ ë‹¨ì–´ì‚¬ì „ìœ¼ë¡œ ì €ì¥í•œë‹¤.  
í•™ìŠµ ì‹œ ì´ ë‹¨ì–´ì‚¬ì „ì„ ê¸°ì¤€ìœ¼ë¡œ í† í°ì„ ë§Œë“¤ê²Œ ëœë‹¤.

## í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬
```python
seq_len = 100
X_data = []
Y_data = []
for i in range(0, n_chars - seq_len, 1):
    seq_in = lower_text[i:i + seq_len]
    seq_out = lower_text[i + seq_len]
    X_data.append([chars_to_int[c] for c in seq_in])
    Y_data.append(chars_to_int[seq_out])
n_patterns = len(X_data)
print("Total Patterns: ", n_patterns)
```
Xì™€ Y ë°ì´í„°ë¥¼ ë§Œë“ ë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì˜ë¼ Xë¥¼ ë§Œë“¤ê³  ê·¸ ìœ„ì— ì˜¤ëŠ” ë‹¨ì¼ ë¬¸ìì—´ í•˜ë‚˜ë¥¼ Yë¡œ ì§€ì •í•´ ë„£ëŠ”ë‹¤. ë‹¨ì–´ ì‚¬ì „ì— ë”°ë¼ ë¬¸ìì—´ì„ ìˆ«ì ë²¡í„°ë¡œ ì „í™˜í•œë‹¤.

```python
X = torch.tensor(X_data, dtype=torch.float32).reshape(n_patterns, seq_len, 1) #tensorë¥¼ í†µí•´ ê° ë¬¸ìë“¤ì„ ë‚˜ëˆ”
X = X / float(n_vocab) #vocabìˆ˜ë¡œ ë‚˜ëˆ ì„œ ì •ê·œí™” (0-1ë¡œ ë§Œë“¦ <- PytorchëŠ” 0~1 ê°’ ì„ í˜¸)
y = torch.tensor(Y_data)
print(X.shape, y.shape)
```
Torchì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ Tensorë¡œ ë°”ê¾¼ ë’¤ì— ì •ê·œí™”ë¥¼ í•œë‹¤.

## ëª¨ë¸ í•™ìŠµ
```python
class BuildModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        self.linear = nn.Linear(256, n_vocab)
    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        x = self.linear(self.dropout(x))
        return x
```
ëª¨ë¸ì„ ë¹Œë“œí•œë‹¤. LSTM, ë“œëì•„ì›ƒ, ì¶œë ¥ì¸µì„ ë§Œë“ ë‹¤.  
ì€ë‹‰ì¸µì€ í•œ ê°œë§Œ ì‚¬ìš©í•œë‹¤.
```python
epochs = 20
batch_size = 128
model = BuildModel()

optimizer = optim.Adam(model.parameters())
loss_fn = nn.CrossEntropyLoss(reduction="sum")
loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)

best_model = None
best_loss = np.inf
for epoch in range(epochs):
    model.train()
    for X_batch, y_batch in loader:
        y_pred = model(X_batch)
        loss = loss_fn(y_pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    #validation
    model.eval()
    loss = 0
    with torch.no_grad():                   #í•™ìŠµì„ ë©ˆì¶”ê³  ëª¨ë¸ í‰ê°€ (validation)
        for X_batch, y_batch in loader:
            y_pred = model(X_batch)
            loss += loss_fn(y_pred, y_batch)
        if loss < best_loss:                #í˜„ì¬ ì €ì¥ëœ lossë³´ë‹¤ ì € ì‘ê²Œ ë‚˜ì˜¤ë©´ ëª¨ë¸ ì €ì¥
            best_loss = loss
            best_model = model.state_dict()
        print('Epoch %d: Cross-entrophy: %.4f' % (epoch, loss))

torch.save([best_model, chars_to_int], "./model_checkpoints/text_generator.pth")
```
í•™ìŠµì„ ì§„í–‰í•œë‹¤. ì˜µí‹°ë§ˆì´ì €ëŠ” Adam, ì†ì‹¤í•¨ìˆ˜ëŠ” Cross Entrophyë¡œ ì§€ì •í•œë‹¤.  
í•œ epochë§ˆë‹¤ loss ê°’ì„ ì¶œë ¥í•´ í•™ìŠµìƒíƒœë¥¼ í‘œì‹œí•œë‹¤.
## í…ìŠ¤íŠ¸ ìƒì„±
```python
best_model, chars_to_int = torch.load("./model_checkpoints/text_generator.pth")
n_vocab = len(chars_to_int)
int_to_chars = dict((i, c) for c, i in chars_to_int.items())

start = np.random.randint(0, len(raw_text)-seq_len)
prompt = lower_text[start:start+seq_len]            #ë¬¸ì„œ ì¤‘ ë¬¸êµ¬ë¥¼ ëœë¤ìœ¼ë¡œ ë½‘ì•„ì„œ seq_length ë§Œí¼ í”„ë¡¬í”„íŠ¸ë¡œ ì…ë ¥
pattern = [chars_to_int[c] for c in prompt]

model.eval()
print("Prompt: \n %s  _____" % prompt)
with torch.no_grad():
    for i in range(1000):           #ëª¨ë¸ì„ 1000ë²ˆ ë°˜ë³µí•¨. ì¦‰ ê¸€ììˆ˜ê°€ 1000ì´ ë ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±
        # ì…ë ¥ì„ tensorë¡œ
        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)
        x = torch.tensor(x, dtype = torch.float32)

        # ë¬¸ì logit ê³„ì‚°
        prediction = model(x)

        # ì¸ë±ìŠ¤ë¥¼ ë¬¸ìë¡œ ë³€í™˜
        index = int(prediction.argmax())
        result = int_to_chars[index]
        print(result, end = "")

        # í”„ë¡¬í”„íŠ¸ì— ë‹¨ì–´ë¥¼ ì¶”ê°€
        pattern.append(index)
        pattern=pattern[1:]
print()
print("__Fin__")
```
<pre>
___<b>í”„ë¡¬í”„íŠ¸</b>
 balloon?"

"yes, i just said to myself coming along: 'i wonder if christopher robin
has such a thing  

___<b>ì¶œë ¥</b>
 and then the boll hs toe tore 

"ioo the tooe   "a larpy hirteey," 
"yhs, yhu kave ho in " 
"the oor toe lo the hertoon " 
"yhs, yhu iave g vorl ho in an hnnettor b aoa oo ho so bo the bootoe " 
"the oere po he poe"thre " soo tere airistopher robin sas shit the borto tf the pore of the sore of the horest, 
"io whu den tooh " 
"yhet so he soo to lave a lort of toeek to an toeetllng "

</pre>
í˜„ì¬ ëª¨ë¸ì€ ì…ë ¥ì´ ì£¼ì–´ì§€ë©´ í•œ ê¸€ìë¥¼ ìƒì„±í•´ë‚´ë¯€ë¡œ ì›í•˜ëŠ” ê¸€ììˆ˜ ë§Œí¼ì˜ ì¶œë ¥ì„ ì–»ê¸°ìœ„í•´ì„œëŠ” ê·¸ë§Œí¼ ëª¨ë¸ì„ ë°˜ë³µí•˜ì—¬ ì‹¤í–‰í•œë‹¤.  
ê° ë°˜ë³µë§ˆë‹¤ ë¡œì§“ì„ ê³„ì‚°í•˜ì—¬ ë¬¸ìë¡œ ë³€í™˜í•œ ë’¤ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•´ ë‹¤ìŒ íšŒì°¨ì—ì„œ ë‹¤ì‹œ ì˜ˆì¸¡ì„ í•œë‹¤.